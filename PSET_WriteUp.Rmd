---
title: "COD Reproducibility Report"
output:
  html_document:
    toc: true
    toc_float: true
---

#### Article ID: hJiYk
#### Pilot: Em Reit
#### Co-pilot: Tom Hardwicke  
#### Start date: 03/12/2017
#### End date: 03/12/2017   

-------

#### Methods summary: 
Sixteen participants were recruited for Study 1. All participants were right-
handed and did not know the purpose of the experiment. Participants either had
their arms crossed or uncrossed, and aligned (close to each other) or misaligned
(far from each other) in the vertical dimension, throughout the study (within-subjects
design.)

Tactile stimuli were trasmitted to the ring finger of each hand in a random
order. Participants had to identify which stimulis was presented first (left or right hand)
by pressing a button with the corresponding hand. 

First, they underwent a practice round of 14 trials, with hands crossed and uncrossed, and aligned and
misaligned. They had to reach 70% accuracy in their reports before the experiment
began. The reaction time, as well as accuracy, were captured in the data.

In Study 1, participants either had their arms uncrossed and aligned (6 cm apart),
crossed and aligned, uncrossed and unaligned (30 cm apart, vertically) or 
crossed and unaligned. Each of the four conditions were presented twice, with 84
trials within each block.
------

#### Target outcomes: 
The target outcomes are as follows:

1. A repeated measure ANOVA, with within-subject factor of *posture* and *alignment*,
showed that the JND in response time was larger when the arms were crossed
as compared to uncrossed. There was a main effect of posture F(1,15)=11.93, p = .004.

2. The cross-hands defecit (difference in performance in the two postures) was 
reduced by a factor of two in the misaligned as compared to the aligned
condition (97.79 and 211.36, respectively). This was further demonstrated
by a signifacnt interaction of posture * alignment, F(1,15)=10.27, p = .006.

3. Despite this reduction, a reliable crossed-hands deficit emerged in both posture
conditions, a planned t-test comparison showed: misaligned: t(15) = 2.81, p = .01,
aligned: t(15) = 3.61, p < .001.

------

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

## Step 1: Load packages

[Some useful packages are being loaded below. You can add any additional ones you might need too.]

```{r}
library(tidyverse) # for data munging
library(plyr)
library(knitr) # for kable table formating
library(haven) # import and export 'SPSS', 'Stata' and 'SAS' Files
library(readxl) # import excel files
library(CODreports) # custom report functions
library(psych)
```


## Step 2: Load data

```{r}
setwd("~/GitHub/set_hJiYk/data")

#the data file I was provided had 6 different tabs. I saved a new file that included just the tab of interest, and load it in here. I have also committed this data file to my repo in case you want to have it too. 

d <- read.csv("study1res.csv", stringsAsFactors = FALSE)
```

## Step 3: Tidy data

```{r}
#renaming column names to be the correct labels
colnames(d)[colnames(d) == 'X'] <- 'subject'
colnames(d)[colnames(d) == 'JND'] <- 'AlignedU'
colnames(d)[colnames(d) == 'X.1'] <- 'AlignedC'
colnames(d)[colnames(d) == 'X.2'] <- 'MisalignedU'
colnames(d)[colnames(d) == 'X.3'] <- 'MisalignedC'

#selecting just the rows and columns that we need.
d1 <- d %>%
  slice(-(1)) %>%
  select(1:5)

d2 <- d1 %>%
  slice(1:16)

d2$AlignedU <- as.numeric(d2$AlignedU)
d2$AlignedC <- as.numeric(d2$AlignedC)
d2$MisalignedU <- as.numeric(d2$MisalignedU)
d2$MisalignedC <- as.numeric(d2$MisalignedC)

#creating crossed-hand deficit scores
d3 <- d2 %>%
  mutate(crossed_deficit_aligned = (AlignedC - AlignedU)) %>%
  mutate(crossed_deficit_misaligned = (MisalignedC - MisalignedU))

d2.tidy <- d2 %>%
  gather(condition, JND, 2:5)

#in tidy form, adding two columns, one that represents posture (crossed v uncrossed),
#and one that represents alignment (aligned v. misaligned).

d2.tidy$posture <- NA
d2.tidy$posture[d2.tidy$condition == "AlignedC"] = "Crossed"     
d2.tidy$posture[d2.tidy$condition == "AlignedU"] = "Uncrossed"     
d2.tidy$posture[d2.tidy$condition == "MisalignedC"] = "Crossed"     
d2.tidy$posture[d2.tidy$condition == "MisalignedU"] = "Uncrossed"     
       
d2.tidy$alignment <- NA
d2.tidy$alignment[d2.tidy$condition == "AlignedC"] = "Aligned"     
d2.tidy$alignment[d2.tidy$condition == "AlignedU"] = "Aligned"     
d2.tidy$alignment[d2.tidy$condition == "MisalignedC"] = "Misaligned"     
d2.tidy$alignment[d2.tidy$condition == "MisalignedU"] = "Misaligned"    

```


## Step 4: Run analysis

### Descriptive statistics

```{r}
aligned_uncrossed_mean <- mean(d2$AlignedU); aligned_uncrossed_mean
aligned_crossed_mean <- mean(d2$AlignedC); aligned_crossed_mean
misaligned_uncrossed_mean <- mean(d2$MisalignedU); misaligned_uncrossed_mean
misaligned_crossed_mean <- mean(d2$MisalignedC); misaligned_crossed_mean
#these match the means in their data
```

### Inferential statistics

```{r}
#TEST 1:

#This is the replication of their first reported outcome. They explain, "A repeated measures ANOVA with the within-subject factors of Posture and Alignment showed that the JND was larger when the arms were crossed as compared to uncrossed (main effect of posture: F(1, 15) = 11.93, p = 0.004." Below, I replicate that test:

rsa <- aov(JND ~ posture * alignment + Error(subject/(posture * alignment)), data = d2.tidy)
summary(rsa)
#The authors do not report including an error variable in their test, but maybe that is supposed to be assumed when running ANOVAs (?) Results of my ANOVA with posture and alignment as within-subjects factors shows that the JND was larger when arms were crossed than uncrossed, F(1,15)= 11.93, p = .004. Thus, their report was correct. 


#TEST 2:

#This is the replication of their second reported outcome. It has two parts. (A) The crossed-hands deficit (difference in performance in the two postures) was reduced by a factor of two in the misaligned as compared to the aligned condition (mean crossed-hands deficit: 96.79 and 211.36 ms, respectively. And (B) The interaction Posture Alignment: F(1,15) = 10.27, p = 0.006)

#Below, I print out the descriptives in the relevant conditions to replicate (A).
describeBy(d2.tidy$JND, group = d2.tidy$condition)

ch_deficit_misaligned <- misaligned_crossed_mean - misaligned_uncrossed_mean; ch_deficit_misaligned
ch_deficit_aligned <- aligned_crossed_mean - aligned_uncrossed_mean; ch_deficit_aligned

#I replicate (A), showing that the crossed-hands deficit is 96.79 in the misaligned condition, and 211.36 in the aligned condition. Their write-up is correct. 

#Below, I attempt eplication (B) - the interaction of posture alignment. This is the same ANOVA as before, but now we focus on the interaction term.
summary(rsa)

#The interaction between posture & alignment is F(1,15)=10.27, p = .006. This also matches what the authors wrote up.


#TEST 3:

#This is a replication of the following results, "However, despite this reduction, a reliable crossed-hands deficit emerged in both conditions (planned t-test comparisons â€“ misaligned: t(15) = 2.81, p = 0.01, dz = 0.70; aligned: t(15) = 3.61, p < 0.003). 

#To do this, I go back to the wide-form data and use the difference scores (i.e., crossed-hands deficit) and compare the difference in them to 0.

t.test(d3$crossed_deficit_misaligned, mu = 0)
t.test(d3$crossed_deficit_aligned, mu = 0)

#In misaligned, I get t(15) = 2.81, p = .013. In aligned, I get t(15) = 3.61, p < .003. Thus, I successfully replicated the authors' findings. 
```


## Step 5: Conclusion

This replication was overall a success. I am confident that I performed Test 1, 2, and 3 correctly (though it took a while to find out exactly what models they ran). But I found no errors in their reporting of these results. 

I should note that, in the final line of their results, they say, "A similar pattern of results was obtained when the proportion of correct responses over all SOA was calculated (main effect of Posture: F(1,15) = 26.47, p < 0.001, gp2 = 0.64; interaction: F(1, 15) = 6.49, p = 0.02, gp2 = 0.30)." This was impossible to replicate, because, if you look at their data, it is separated into a long-form "raw" sheet that includes SOA but **does not include JND or any reaction time** (?? I thought this to be weird.) And it includes a cumulative sheet that has the mean JDN and accuracy per subject. But there was no way to match the individual trials on the raw data to the output in the summary sheet data, so I could not do anything with SOA. So although these data are "publicly available," it seems that the raw data should also have individual response time or JND times in it, because as it is it is only collapsed across subject & condition, and there is no way to back it out. 

[This function will output information about the package versions used in this report:]

```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```
